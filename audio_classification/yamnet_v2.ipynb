{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/1_dqbwsn08sbtf7s0k5q2lsc0000gn/T/ipykernel_59475/4222563939.py:9: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from IPython.core.display import display\n",
    "from scipy.io import wavfile\n",
    "import scipy\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# LSTM\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Activation, MaxPool2D, Flatten, Dropout\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import librosa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(CURR_DIR, \"huge_collated_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample waveform to 16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_sample_rate(original_sample_rate, waveform,\n",
    "                       desired_sample_rate=16000):\n",
    "  \"\"\"Resample waveform if required.\"\"\"\n",
    "  if original_sample_rate != desired_sample_rate:\n",
    "    desired_length = int(round(float(len(waveform)) /\n",
    "                               original_sample_rate * desired_sample_rate))\n",
    "    waveform = scipy.signal.resample(waveform, desired_length)\n",
    "  return desired_sample_rate, waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_wavfile(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 156.07553925, -253.42262954, -112.00799594, ...,   90.55064662,\n",
       "         74.79091666,  -50.709726  ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 16000 Hz\n",
      "Total duration: 3.52s\n",
      "Size of the input: 56287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/1_dqbwsn08sbtf7s0k5q2lsc0000gn/T/ipykernel_59475/788395808.py:7: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, wav_data = wavfile.read(file_path, 'rb')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd03c069400>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wav_file_name = 'speech_whistling2.wav'\n",
    "category = \"happy\"\n",
    "wav_file_name = '2c9c46babe.wav'\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, category, wav_file_name)\n",
    "\n",
    "sample_rate, wav_data = wavfile.read(file_path, 'rb')\n",
    "sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "\n",
    "# Show some basic information about the audio.\n",
    "duration = len(wav_data)/sample_rate\n",
    "print(f'Sample rate: {sample_rate} Hz')\n",
    "print(f'Total duration: {duration:.2f}s')\n",
    "print(f'Size of the input: {len(wav_data)}')\n",
    "\n",
    "# Listening to the wav file.\n",
    "Audio(wav_data, rate=sample_rate)\n",
    "plt.plot(wav_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 16000 Hz\n",
      "Total duration: 2.17s\n",
      "Size of the input: 34790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8c0c80f1c0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wav_file_name = 'speech_whistling2.wav'\n",
    "category = \"sad\"\n",
    "wav_file_name = '1ac2b3d011.wav'\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, category, wav_file_name)\n",
    "\n",
    "sample_rate, wav_data = wavfile.read(file_path, 'rb')\n",
    "sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "\n",
    "# Show some basic information about the audio.\n",
    "duration = len(wav_data)/sample_rate\n",
    "print(f'Sample rate: {sample_rate} Hz')\n",
    "print(f'Total duration: {duration:.2f}s')\n",
    "print(f'Size of the input: {len(wav_data)}')\n",
    "\n",
    "# Listening to the wav file.\n",
    "Audio(wav_data, rate=sample_rate)\n",
    "plt.plot(wav_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 16000 Hz\n",
      "Total duration: 2.11s\n",
      "Size of the input: 33717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8bf9a69670>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wav_file_name = 'speech_whistling2.wav'\n",
    "category = \"neutral\"\n",
    "wav_file_name = '3a49f17ceb.wav'\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, category, wav_file_name)\n",
    "\n",
    "sample_rate, wav_data = wavfile.read(file_path, 'rb')\n",
    "sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "\n",
    "# Show some basic information about the audio.\n",
    "duration = len(wav_data)/sample_rate\n",
    "print(f'Sample rate: {sample_rate} Hz')\n",
    "print(f'Total duration: {duration:.2f}s')\n",
    "print(f'Size of the input: {len(wav_data)}')\n",
    "\n",
    "# Listening to the wav file.\n",
    "Audio(wav_data, rate=sample_rate)\n",
    "plt.plot(wav_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 16000 Hz\n",
      "Total duration: 1.74s\n",
      "Size of the input: 27816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8bf8ede0a0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wav_file_name = 'speech_whistling2.wav'\n",
    "category = \"fear\"\n",
    "wav_file_name = '0ef63f4364.wav'\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, category, wav_file_name)\n",
    "\n",
    "sample_rate, wav_data = wavfile.read(file_path, 'rb')\n",
    "sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "\n",
    "# Show some basic information about the audio.\n",
    "duration = len(wav_data)/sample_rate\n",
    "print(f'Sample rate: {sample_rate} Hz')\n",
    "print(f'Total duration: {duration:.2f}s')\n",
    "print(f'Size of the input: {len(wav_data)}')\n",
    "\n",
    "# Listening to the wav file.\n",
    "Audio(wav_data, rate=sample_rate)\n",
    "plt.plot(wav_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'sad', 'fear', 'neutral', 'angry']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = []\n",
    "for category in os.listdir(DATA_DIR):\n",
    "    if not category.startswith(\".\"):\n",
    "        categories.append(category)\n",
    "\n",
    "# categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/daniel/Documents/GitHub/pentahack/audio_classification/huge_collated_dataset'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for category in categories[:]:\n",
    "    CATEGORY_DIR = os.path.join(DATA_DIR, category)\n",
    "\n",
    "    for file in os.listdir(CATEGORY_DIR):\n",
    "        if file.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(CATEGORY_DIR, file)        \n",
    "        rel_file_path = os.path.relpath(file_path)\n",
    "\n",
    "        row = {\n",
    "            \"file\": file,\n",
    "            \"file_path\": rel_file_path, \n",
    "            \"label\": category\n",
    "        }\n",
    "    \n",
    "        rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy_0_0_78_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_0_0_78_vesus...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy_5_2_124_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_5_2_124_vesu...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy_7_4_202_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_7_4_202_vesu...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy_4_4_38_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_4_4_38_vesus...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy_6_4_140_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_6_4_140_vesu...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      file                                          file_path  \\\n",
       "0   happy_0_0_78_vesus.wav  huge_collated_dataset/happy/happy_0_0_78_vesus...   \n",
       "1  happy_5_2_124_vesus.wav  huge_collated_dataset/happy/happy_5_2_124_vesu...   \n",
       "2  happy_7_4_202_vesus.wav  huge_collated_dataset/happy/happy_7_4_202_vesu...   \n",
       "3   happy_4_4_38_vesus.wav  huge_collated_dataset/happy/happy_4_4_38_vesus...   \n",
       "4  happy_6_4_140_vesus.wav  huge_collated_dataset/happy/happy_6_4_140_vesu...   \n",
       "\n",
       "   label  \n",
       "0  happy  \n",
       "1  happy  \n",
       "2  happy  \n",
       "3  happy  \n",
       "4  happy  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8577</td>\n",
       "      <td>8577</td>\n",
       "      <td>8577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8577</td>\n",
       "      <td>8577</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>happy_0_0_78_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_0_0_78_vesus...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file  \\\n",
       "count                     8577   \n",
       "unique                    8577   \n",
       "top     happy_0_0_78_vesus.wav   \n",
       "freq                         1   \n",
       "\n",
       "                                                file_path    label  \n",
       "count                                                8577     8577  \n",
       "unique                                               8577        5  \n",
       "top     huge_collated_dataset/happy/happy_0_0_78_vesus...  neutral  \n",
       "freq                                                    1     1969  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save datafram\n",
    "# df.to_csv(\"huge_collated_dataset_meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = wav_data / tf.int16.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_target = {\n",
    "    \"sad\": 0,\n",
    "    \"fear\": 1,\n",
    "    \"angry\": 2,\n",
    "    \"neutral\": 3, \n",
    "    \"happy\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8577\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy_0_0_78_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_0_0_78_vesus...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy_5_2_124_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_5_2_124_vesu...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy_7_4_202_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_7_4_202_vesu...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy_4_4_38_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_4_4_38_vesus...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy_6_4_140_vesus.wav</td>\n",
       "      <td>huge_collated_dataset/happy/happy_6_4_140_vesu...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      file                                          file_path  \\\n",
       "0   happy_0_0_78_vesus.wav  huge_collated_dataset/happy/happy_0_0_78_vesus...   \n",
       "1  happy_5_2_124_vesus.wav  huge_collated_dataset/happy/happy_5_2_124_vesu...   \n",
       "2  happy_7_4_202_vesus.wav  huge_collated_dataset/happy/happy_7_4_202_vesu...   \n",
       "3   happy_4_4_38_vesus.wav  huge_collated_dataset/happy/happy_4_4_38_vesus...   \n",
       "4  happy_6_4_140_vesus.wav  huge_collated_dataset/happy/happy_6_4_140_vesu...   \n",
       "\n",
       "   label  \n",
       "0  happy  \n",
       "1  happy  \n",
       "2  happy  \n",
       "3  happy  \n",
       "4  happy  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the meta data\n",
    "df = pd.read_csv(\"huge_collated_dataset_meta.csv\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['label'].apply(lambda x: label_to_target[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4\n",
       "1       4\n",
       "2       4\n",
       "3       4\n",
       "4       4\n",
       "       ..\n",
       "8572    2\n",
       "8573    2\n",
       "8574    2\n",
       "8575    2\n",
       "8576    2\n",
       "Name: target, Length: 8577, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file         0\n",
       "file_path    0\n",
       "label        0\n",
       "target       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['file_path'], df['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, label):\n",
    "    # file_path_str = bytes.decode(file_path.numpy())\n",
    "    # print(file_path_str)\n",
    "    sample_rate, wav_data = wavfile.read(file_path, 'rb')\n",
    "    sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "\n",
    "    # # Normalise wave data\n",
    "    # wav_data = wav_data / tf.int16.max\n",
    "\n",
    "    # Convert to tensor\n",
    "    wav_data_tensor = tf.convert_to_tensor(wav_data, dtype=tf.float32)\n",
    "    label_tensor = tf.convert_to_tensor(label, dtype=tf.int64)\n",
    "    return (wav_data_tensor, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 01:08:11.395741: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-04-07 01:08:11.422731: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-04-07 01:08:11.430293: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(wav_data, label):\n",
    "  ''' run YAMNet to extract embedding from the wav data '''\n",
    "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "  num_embeddings = tf.shape(embeddings)[0]\n",
    "  return (embeddings,\n",
    "            tf.repeat(label, num_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, embeddings, spectrogram = yamnet_model(wav_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/1_dqbwsn08sbtf7s0k5q2lsc0000gn/T/ipykernel_59475/2950063737.py:4: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, wav_data = wavfile.read(file_path, 'rb')\n"
     ]
    }
   ],
   "source": [
    "for file_path, label in list(zip(X_train, y_train)):\n",
    "    wav_data_tensor, label_tensor = load_audio(file_path, label)\n",
    "    train_ls.append((wav_data_tensor, label_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_tensor_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave, label in train_ls:\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "    for embedding in embeddings:\n",
    "        train_embedding_tensor_ls.append((embedding, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_wave_tensor_ds = tf.concat(train_ls, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save train_embedding_tensor_ls\n",
    "# with open(\"train_embeeding_tensor_ls.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(train_embedding_tensor_ls, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train_embedding_tensor_ls\n",
    "with open(\"train_embeeding_tensor_ls.pkl\", \"rb\") as file:\n",
    "    train_embedding_tensor_ls = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48027"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_embedding_tensor_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/1_dqbwsn08sbtf7s0k5q2lsc0000gn/T/ipykernel_59475/2950063737.py:4: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, wav_data = wavfile.read(file_path, 'rb')\n"
     ]
    }
   ],
   "source": [
    "for file_path, label in list(zip(X_test, y_test)):\n",
    "    wav_data_tensor, label_tensor = load_audio(file_path, label)\n",
    "    test_ls.append((wav_data_tensor, label_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_tensor_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave, label in test_ls:\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "    for embedding in embeddings:\n",
    "        test_embedding_tensor_ls.append((embedding, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save train_embedding_tensor_ls\n",
    "# with open(\"test_embeeding_tensor_ls.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(test_embedding_tensor_ls, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train_embedding_tensor_ls\n",
    "with open(\"test_embeeding_tensor_ls.pkl\", \"rb\") as file:\n",
    "    test_embedding_tensor_ls = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12012"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_embedding_tensor_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Pack as input #1(zero-based) was expected to be a float tensor but is a int64 tensor [Op:Pack] name: packed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb Cell 61\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb#Y254sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tf\u001b[39m.\u001b[39;49mconcat(test_embedding_tensor_ls, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1677\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m   1674\u001b[0m         axis, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconcat_dim\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39massert_has_rank(\u001b[39m0\u001b[39m)\n\u001b[1;32m   1676\u001b[0m     \u001b[39mreturn\u001b[39;00m identity(values[\u001b[39m0\u001b[39m], name\u001b[39m=\u001b[39mname)\n\u001b[0;32m-> 1677\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mconcat_v2(values\u001b[39m=\u001b[39;49mvalues, axis\u001b[39m=\u001b[39;49maxis, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:1197\u001b[0m, in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1195\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1197\u001b[0m   \u001b[39mreturn\u001b[39;00m concat_v2_eager_fallback(\n\u001b[1;32m   1198\u001b[0m       values, axis, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n\u001b[1;32m   1199\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n\u001b[1;32m   1200\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:1228\u001b[0m, in \u001b[0;36mconcat_v2_eager_fallback\u001b[0;34m(values, axis, name, ctx)\u001b[0m\n\u001b[1;32m   1224\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1225\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mExpected list for \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1226\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconcat_v2\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Op, not \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m values)\n\u001b[1;32m   1227\u001b[0m _attr_N \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(values)\n\u001b[0;32m-> 1228\u001b[0m _attr_T, values \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49margs_to_matching_eager(\u001b[39mlist\u001b[39;49m(values), ctx, [])\n\u001b[1;32m   1229\u001b[0m _attr_Tidx, (axis,) \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39margs_to_matching_eager([axis], ctx, [_dtypes\u001b[39m.\u001b[39mint32, _dtypes\u001b[39m.\u001b[39mint64, ], _dtypes\u001b[39m.\u001b[39mint32)\n\u001b[1;32m   1230\u001b[0m _inputs_flat \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(values) \u001b[39m+\u001b[39m [axis]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:273\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[1;32m    270\u001b[0m     tensor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m   tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[1;32m    274\u001b[0m       t, dtype, preferred_dtype\u001b[39m=\u001b[39;49mdefault_dtype, ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    276\u001b[0m ret\u001b[39m.\u001b[39mappend(tensor)\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1540\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1535\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1536\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1537\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1540\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1543\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1525\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[39melif\u001b[39;00m dtype \u001b[39m!=\u001b[39m inferred_dtype:\n\u001b[1;32m   1524\u001b[0m   v \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(_cast_nested_seqs_to_dtype(dtype), v)\n\u001b[0;32m-> 1525\u001b[0m \u001b[39mreturn\u001b[39;00m _autopacking_helper(v, dtype, name \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mpacked\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1431\u001b[0m, in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   1428\u001b[0m   \u001b[39m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m   \u001b[39m# checking.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, core\u001b[39m.\u001b[39mTensor) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m list_or_tuple):\n\u001b[0;32m-> 1431\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mpack(list_or_tuple, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1432\u001b[0m must_pack \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m converted_elems \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:6380\u001b[0m, in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6378\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6379\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 6380\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   6381\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   6382\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6862\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m message \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6861\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 6862\u001b[0m six\u001b[39m.\u001b[39;49mraise_from(core\u001b[39m.\u001b[39;49m_status_to_exception(e\u001b[39m.\u001b[39;49mcode, message), \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Pack as input #1(zero-based) was expected to be a float tensor but is a int64 tensor [Op:Pack] name: packed"
     ]
    }
   ],
   "source": [
    "tf.concat(test_embedding_tensor_ls, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = []\n",
    "train_labels = []\n",
    "for tensor, label in train_embedding_tensor_ls:\n",
    "    # np_array = tensor.numpy()\n",
    "    train_tensor.append(tensor)\n",
    "    train_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensors((train_tensor, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = []\n",
    "test_labels = []\n",
    "for tensor, label in test_embedding_tensor_ls:\n",
    "    # np_array = tensor.numpy()\n",
    "    test_tensor.append(tensor)\n",
    "    test_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensors((test_tensor, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48027, 1024) tf.Tensor([0 0 0 ... 2 2 2], shape=(48027,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter = 0 \n",
    "for embedding, label in train_dataset:\n",
    "    if counter > 5:\n",
    "        break\n",
    "    \n",
    "    print(embedding.shape, label)\n",
    "\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorDataset shapes: ((48027, 1024), (48027,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 591,109\n",
      "Trainable params: 591,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# class Model(tf.Module):\n",
    "#     def __init__(self, name=None):\n",
    "#         super().__init__(name=name)\n",
    "#         self.dense1 = tf.keras.layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024,), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(5)\n",
    "], name='my_model')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=5,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "save_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'model_1/model.epoch{epoch:02d}.hdf5',\n",
    "    monitor ='val_loss',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    ")\n",
    "\n",
    "callbacks = [early_stop, save_model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5255 - accuracy: 0.1899 - val_loss: 12.0685 - val_accuracy: 0.2261\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 12.06854, saving model to model_1/model.epoch01.hdf5\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 12.0181 - accuracy: 0.2304 - val_loss: 13.6940 - val_accuracy: 0.1999\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 12.06854\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 13.6227 - accuracy: 0.2121 - val_loss: 11.4870 - val_accuracy: 0.1863\n",
      "\n",
      "Epoch 00003: val_loss improved from 12.06854 to 11.48698, saving model to model_1/model.epoch03.hdf5\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.6880 - accuracy: 0.1833 - val_loss: 12.0986 - val_accuracy: 0.2063\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 11.48698\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 12.4055 - accuracy: 0.1857 - val_loss: 7.6986 - val_accuracy: 0.2060\n",
      "\n",
      "Epoch 00005: val_loss improved from 11.48698 to 7.69864, saving model to model_1/model.epoch05.hdf5\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.8547 - accuracy: 0.1925 - val_loss: 7.7377 - val_accuracy: 0.2143\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 7.69864\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                       epochs=20,\n",
    "                       validation_data=test_dataset,\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MOdel v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Pack as input #1(zero-based) was expected to be a float tensor but is a int64 tensor [Op:Pack] name: packed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_wave_tensor_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconcat(train_ls, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    199\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 201\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "\u001b[1;32m    203\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n",
      "\u001b[1;32m    204\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n",
      "\u001b[1;32m    205\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1677\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n",
      "\u001b[1;32m   1673\u001b[0m     ops\u001b[39m.\u001b[39mconvert_to_tensor(\n",
      "\u001b[1;32m   1674\u001b[0m         axis, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconcat_dim\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[1;32m   1675\u001b[0m         dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39massert_has_rank(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32m   1676\u001b[0m     \u001b[39mreturn\u001b[39;00m identity(values[\u001b[39m0\u001b[39m], name\u001b[39m=\u001b[39mname)\n",
      "\u001b[0;32m-> 1677\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mconcat_v2(values\u001b[39m=\u001b[39;49mvalues, axis\u001b[39m=\u001b[39;49maxis, name\u001b[39m=\u001b[39;49mname)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:1197\u001b[0m, in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n",
      "\u001b[1;32m   1195\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "\u001b[1;32m   1196\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1197\u001b[0m   \u001b[39mreturn\u001b[39;00m concat_v2_eager_fallback(\n",
      "\u001b[1;32m   1198\u001b[0m       values, axis, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n",
      "\u001b[1;32m   1199\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n",
      "\u001b[1;32m   1200\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:1228\u001b[0m, in \u001b[0;36mconcat_v2_eager_fallback\u001b[0;34m(values, axis, name, ctx)\u001b[0m\n",
      "\u001b[1;32m   1224\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n",
      "\u001b[1;32m   1225\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mExpected list for \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument to \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   1226\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconcat_v2\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Op, not \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m values)\n",
      "\u001b[1;32m   1227\u001b[0m _attr_N \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(values)\n",
      "\u001b[0;32m-> 1228\u001b[0m _attr_T, values \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49margs_to_matching_eager(\u001b[39mlist\u001b[39;49m(values), ctx, [])\n",
      "\u001b[1;32m   1229\u001b[0m _attr_Tidx, (axis,) \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39margs_to_matching_eager([axis], ctx, [_dtypes\u001b[39m.\u001b[39mint32, _dtypes\u001b[39m.\u001b[39mint64, ], _dtypes\u001b[39m.\u001b[39mint32)\n",
      "\u001b[1;32m   1230\u001b[0m _inputs_flat \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(values) \u001b[39m+\u001b[39m [axis]\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:273\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n",
      "\u001b[1;32m    270\u001b[0m     tensor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 273\u001b[0m   tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n",
      "\u001b[1;32m    274\u001b[0m       t, dtype, preferred_dtype\u001b[39m=\u001b[39;49mdefault_dtype, ctx\u001b[39m=\u001b[39;49mctx)\n",
      "\u001b[1;32m    276\u001b[0m ret\u001b[39m.\u001b[39mappend(tensor)\n",
      "\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    161\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n",
      "\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1540\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n",
      "\u001b[1;32m   1535\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   1536\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n",
      "\u001b[1;32m   1537\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n",
      "\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m-> 1540\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n",
      "\u001b[1;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n",
      "\u001b[1;32m   1543\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1525\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n",
      "\u001b[1;32m   1523\u001b[0m \u001b[39melif\u001b[39;00m dtype \u001b[39m!=\u001b[39m inferred_dtype:\n",
      "\u001b[1;32m   1524\u001b[0m   v \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(_cast_nested_seqs_to_dtype(dtype), v)\n",
      "\u001b[0;32m-> 1525\u001b[0m \u001b[39mreturn\u001b[39;00m _autopacking_helper(v, dtype, name \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mpacked\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1431\u001b[0m, in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n",
      "\u001b[1;32m   1427\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n",
      "\u001b[1;32m   1428\u001b[0m   \u001b[39m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[39;00m\n",
      "\u001b[1;32m   1429\u001b[0m   \u001b[39m# checking.\u001b[39;00m\n",
      "\u001b[1;32m   1430\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, core\u001b[39m.\u001b[39mTensor) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m list_or_tuple):\n",
      "\u001b[0;32m-> 1431\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mpack(list_or_tuple, name\u001b[39m=\u001b[39;49mname)\n",
      "\u001b[1;32m   1432\u001b[0m must_pack \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m   1433\u001b[0m converted_elems \u001b[39m=\u001b[39m []\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:6380\u001b[0m, in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n",
      "\u001b[1;32m   6378\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n",
      "\u001b[1;32m   6379\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m-> 6380\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n",
      "\u001b[1;32m   6381\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n",
      "\u001b[1;32m   6382\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6862\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n",
      "\u001b[1;32m   6860\u001b[0m message \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m   6861\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[0;32m-> 6862\u001b[0m six\u001b[39m.\u001b[39;49mraise_from(core\u001b[39m.\u001b[39;49m_status_to_exception(e\u001b[39m.\u001b[39;49mcode, message), \u001b[39mNone\u001b[39;49;00m)\n",
      "\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Pack as input #1(zero-based) was expected to be a float tensor but is a int64 tensor [Op:Pack] name: packed"
     ]
    }
   ],
   "source": [
    "# train_wave_tensor_ds = tf.concat(train_ls, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train_embedding_tensor_ls\n",
    "with open(\"train_embeeding_tensor_ls.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_embedding_tensor_ls, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute Pack as input #1(zero-based) was expected to be a float tensor but is a int64 tensor [Op:Pack] name: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb#Y161sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(train_ls)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1404\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   1341\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1343\u001b[0m     value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1344\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \n\u001b[1;32m   1346\u001b[0m \u001b[39m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[1;32m   1405\u001b[0m       value, dtype\u001b[39m=\u001b[39;49mdtype, dtype_hint\u001b[39m=\u001b[39;49mdtype_hint, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1410\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1409\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1410\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor(\n\u001b[1;32m   1411\u001b[0m       value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m   1412\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1413\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1414\u001b[0m       preferred_dtype\u001b[39m=\u001b[39;49mdtype_hint,\n\u001b[1;32m   1415\u001b[0m       as_ref\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1540\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1535\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1536\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1537\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1540\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1543\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1525\u001b[0m, in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[39melif\u001b[39;00m dtype \u001b[39m!=\u001b[39m inferred_dtype:\n\u001b[1;32m   1524\u001b[0m   v \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(_cast_nested_seqs_to_dtype(dtype), v)\n\u001b[0;32m-> 1525\u001b[0m \u001b[39mreturn\u001b[39;00m _autopacking_helper(v, dtype, name \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mpacked\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1444\u001b[0m, in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1442\u001b[0m   must_pack \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m-> 1444\u001b[0m   converted_elem \u001b[39m=\u001b[39m _autopacking_helper(elem, dtype, \u001b[39mstr\u001b[39;49m(i))\n\u001b[1;32m   1445\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(converted_elem, core\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m   1446\u001b[0m     must_pack \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1431\u001b[0m, in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   1428\u001b[0m   \u001b[39m# NOTE: Fast path when all the items are tensors, this doesn't do any type\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m   \u001b[39m# checking.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, core\u001b[39m.\u001b[39mTensor) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m list_or_tuple):\n\u001b[0;32m-> 1431\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mpack(list_or_tuple, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1432\u001b[0m must_pack \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m converted_elems \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:6380\u001b[0m, in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6378\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6379\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 6380\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   6381\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   6382\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6862\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m message \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6861\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 6862\u001b[0m six\u001b[39m.\u001b[39;49mraise_from(core\u001b[39m.\u001b[39;49m_status_to_exception(e\u001b[39m.\u001b[39;49mcode, message), \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Pack as input #1(zero-based) was expected to be a float tensor but is a int64 tensor [Op:Pack] name: 0"
     ]
    }
   ],
   "source": [
    "tf.convert_to_tensor(train_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/1_dqbwsn08sbtf7s0k5q2lsc0000gn/T/ipykernel_59475/2950063737.py:4: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, wav_data = wavfile.read(file_path, 'rb')\n"
     ]
    }
   ],
   "source": [
    "for file_path, label in list(zip(X_train, y_train)):\n",
    "    wav_data_tensor, label_tensor = load_audio(file_path, label)\n",
    "    train_ls.append((wav_data_tensor, label_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "OpKernel 'ConcatV2' has constraint on attr 'T' not in NodeDef '[N=0, Tidx=DT_INT32]', KernelDef: 'op: \"ConcatV2\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT64 } } } host_memory_arg: \"axis\"' [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_wave_tensor_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconcat(train_ls, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    199\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 201\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "\u001b[1;32m    203\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n",
      "\u001b[1;32m    204\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n",
      "\u001b[1;32m    205\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1677\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n",
      "\u001b[1;32m   1673\u001b[0m     ops\u001b[39m.\u001b[39mconvert_to_tensor(\n",
      "\u001b[1;32m   1674\u001b[0m         axis, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconcat_dim\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[1;32m   1675\u001b[0m         dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39massert_has_rank(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32m   1676\u001b[0m     \u001b[39mreturn\u001b[39;00m identity(values[\u001b[39m0\u001b[39m], name\u001b[39m=\u001b[39mname)\n",
      "\u001b[0;32m-> 1677\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mconcat_v2(values\u001b[39m=\u001b[39;49mvalues, axis\u001b[39m=\u001b[39;49maxis, name\u001b[39m=\u001b[39;49mname)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:1193\u001b[0m, in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n",
      "\u001b[1;32m   1191\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n",
      "\u001b[1;32m   1192\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m-> 1193\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n",
      "\u001b[1;32m   1194\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n",
      "\u001b[1;32m   1195\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6862\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n",
      "\u001b[1;32m   6860\u001b[0m message \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m   6861\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[0;32m-> 6862\u001b[0m six\u001b[39m.\u001b[39;49mraise_from(core\u001b[39m.\u001b[39;49m_status_to_exception(e\u001b[39m.\u001b[39;49mcode, message), \u001b[39mNone\u001b[39;49;00m)\n",
      "\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: OpKernel 'ConcatV2' has constraint on attr 'T' not in NodeDef '[N=0, Tidx=DT_INT32]', KernelDef: 'op: \"ConcatV2\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT64 } } } host_memory_arg: \"axis\"' [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "train_wave_tensor_ds = tf.concat(train_ls, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save wave \n",
    "with open(\"train_wave_tensor\", \"wb\") as file:\n",
    "    pickle.dump(train_ls, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ls[2][1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the train and test set\n",
    "# df_train = pd.concat([X_train, y_train], axis=1)\n",
    "# print(df_train.head())\n",
    "\n",
    "# df_train.to_csv(\"df_train.csv\", index=False)\n",
    "\n",
    "# df_test = pd.concat([X_test, y_test], axis=1)\n",
    "# print(df_test.head())\n",
    "\n",
    "# df_test.to_csv(\"df_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav1 = tf.convert_to_tensor(wav_data)\n",
    "wav2 = tf.convert_to_tensor(wav_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [wav1, wav2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "waves = tf.stack(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 27520])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2087      huge_collated_dataset/sad/sad_2_5_174_vesus.wav\n",
       "5848    huge_collated_dataset/neutral/neutral_5_5_108_...\n",
       "8532    huge_collated_dataset/angry/angry_7_6_78_vesus...\n",
       "96      huge_collated_dataset/happy/happy_4_4_206_vesu...\n",
       "5751    huge_collated_dataset/neutral/neutral_9_6_33_v...\n",
       "                              ...                        \n",
       "5734    huge_collated_dataset/neutral/neutral_1_3_229_...\n",
       "5191    huge_collated_dataset/neutral/neutral_5_5_85_v...\n",
       "5390    huge_collated_dataset/neutral/neutral_8_1_145_...\n",
       "860     huge_collated_dataset/happy/happy_2_2_143_vesu...\n",
       "7270    huge_collated_dataset/angry/angry_4_1_92_vesus...\n",
       "Name: file_path, Length: 6861, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Tensor Slices Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_target = {\n",
    "    \"sad\": 0,\n",
    "    \"fear\": 1,\n",
    "    \"angry\": 2,\n",
    "    \"neutral\": 3, \n",
    "    \"happy\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"df_train.csv\")\n",
    "df_test = pd.read_csv(\"df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['file_path'][450:]\n",
    "y_train = df_train['label'][450:].apply(lambda x: label_to_target[x])\n",
    "X_test = df_test['file_path']\n",
    "y_test = df_test['label'].apply(lambda x: label_to_target[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper load audio function\n",
    "# @tf.function\n",
    "def load_audio(file_path, label):\n",
    "    file_path_str = bytes.decode(file_path.numpy())\n",
    "    print(file_path_str)\n",
    "    sample_rate, wav_data = wavfile.read(file_path_str, 'rb')\n",
    "    sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
    "\n",
    "    # Normalise wave data\n",
    "    wav_data = wav_data / tf.int16.max\n",
    "\n",
    "    # Convert to tensor\n",
    "    wav_data_tensor = tf.convert_to_tensor(wav_data, dtype=tf.float32)\n",
    "    return (wav_data_tensor, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_audio_ds = train_ds.map(load_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_audio_ds = train_ds.map(lambda x, y: tf.py_function(load_audio, [x, y], [tf.float32, tf.int64]))\n",
    "train_audio_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=<unknown>, dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_audio_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_audio_ds = test_ds.map(lambda x, y: tf.py_function(load_audio, [x, y], [tf.float32, tf.int64]))\n",
    "test_audio_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=<unknown>, dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_audio_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for wave, target in train_audio_ds:\n",
    "#     print(wave[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with YAMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 02:04:28.529390: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-04-07 02:04:28.798051: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(wav_data, label):\n",
    "  ''' run YAMNet to extract embedding from the wav data '''\n",
    "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "  num_embeddings = tf.shape(embeddings)[0]\n",
    "  return (embeddings,\n",
    "            tf.repeat(label, num_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds = train_audio_ds.map(extract_embedding).unbatch()\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = test_audio_ds.map(extract_embedding).unbatch()\n",
    "val_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the main_ds\n",
    "# tf.data.experimental.save(main_ds, \"./main_ds\")\n",
    "\n",
    "# # Save the val_ds\n",
    "# tf.data.experimental.save(val_ds, \"./val_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train_ds = main_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_train_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "# test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 591,109\n",
      "Trainable params: 591,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# class Model(tf.Module):\n",
    "#     def __init__(self, name=None):\n",
    "#         super().__init__(name=name)\n",
    "#         self.dense1 = tf.keras.layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(5)\n",
    "], name='my_model')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "save_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'model_1/model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5',\n",
    "    monitor ='val_loss',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    ")\n",
    "\n",
    "callbacks = [early_stop, save_model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "huge_collated_dataset/fear/41e5079eac.wav\n",
      "huge_collated_dataset/sad/sad_3_1_22_vesus.wav\n",
      "huge_collated_dataset/sad/sad_2_5_128_vesus.wav\n",
      "huge_collated_dataset/happy/happy_6_4_188_vesus.wav\n",
      "huge_collated_dataset/sad/sad_4_5_11_vesus.wav\n",
      "huge_collated_dataset/sad/sad_0_1_4_vesus.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/1_dqbwsn08sbtf7s0k5q2lsc0000gn/T/ipykernel_59475/1113092067.py:6: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, wav_data = wavfile.read(file_path_str, 'rb')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huge_collated_dataset/angry/angry_2_0_140_vesus.wav\n",
      "huge_collated_dataset/neutral/neutral_7_1_89_vesus.wav\n",
      "huge_collated_dataset/neutral/neutral_4_0_191_vesus.wav\n",
      "huge_collated_dataset/neutral/neutral_3_3_31_vesus.wav\n",
      "huge_collated_dataset/happy/happy_3_0_212_vesus.wav\n",
      "huge_collated_dataset/fear/fear_ravdess1187.wav\n",
      "huge_collated_dataset/neutral/neutral_9_6_40_vesus.wav\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " The first dimension of paddings must be the rank of inputs[1,2] [69936,2]\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/yamnet_frames/tf_op_layer_Pad/Pad}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_570947]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb Cell 118\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(main_train_ds,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                        epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                        validation_data\u001b[39m=\u001b[39;49mval_train_ds,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v2.ipynb#Y111sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                        callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1101\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name) \u001b[39mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    829\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:888\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[0;32m--> 888\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    889\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m   _, _, _, filtered_flat_args \u001b[39m=\u001b[39m \\\n\u001b[1;32m    891\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    892\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2943\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1919\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    556\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    557\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    558\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    559\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    560\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    561\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  The first dimension of paddings must be the rank of inputs[1,2] [69936,2]\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/yamnet_frames/tf_op_layer_Pad/Pad}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_570947]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(main_train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_train_ds,\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, axis=0, **kwargs):\n",
    "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "    self.axis = axis\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wave_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v1.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v1.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run the model, check the output.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/yamnet_v1.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m scores, embeddings, spectrogram \u001b[39m=\u001b[39m model(wave_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wave_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the model, check the output.\n",
    "scores, embeddings, spectrogram = model(wave_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 19:28:21.324701: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# Run the model, check the output.\n",
    "scores, embeddings, spectrogram = model(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1024), dtype=float32, numpy=\n",
       "array([[2.175924  , 0.3591828 , 0.5648943 , ..., 0.34278473, 2.2839918 ,\n",
       "        3.6191835 ],\n",
       "       [2.3264675 , 0.41704032, 0.45783436, ..., 0.33244064, 2.3892274 ,\n",
       "        3.5955532 ],\n",
       "       [2.0903363 , 0.34951845, 0.65777   , ..., 0.35120144, 2.218155  ,\n",
       "        3.730153  ]], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1024), dtype=float32, numpy=\n",
       "array([[2.175924  , 0.3591828 , 0.5648943 , ..., 0.34278473, 2.2839918 ,\n",
       "        3.6191835 ],\n",
       "       [2.3264675 , 0.41704032, 0.45783436, ..., 0.33244064, 2.3892274 ,\n",
       "        3.5955532 ],\n",
       "       [2.0903363 , 0.34951845, 0.65777   , ..., 0.35120144, 2.218155  ,\n",
       "        3.730153  ]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the waveform.\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(waveform)\n",
    "plt.xlim([0, len(waveform)])\n",
    "\n",
    "# Plot the log-mel spectrogram (returned by the model).\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')\n",
    "\n",
    "# Plot and label the model output scores for the top-scoring classes.\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "top_n = 10\n",
    "top_class_indices = np.argsort(mean_scores)[::-1][:top_n]\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
    "\n",
    "# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n",
    "# values from the model documentation\n",
    "patch_padding = (0.025 / 2) / 0.01\n",
    "plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])\n",
    "# Label the top_N classes.\n",
    "yticks = range(0, top_n, 1)\n",
    "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
    "_ = plt.ylim(-0.5 + np.array([top_n, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('carrosell2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8501986c27cdf10dcff24130cd3590078fda1030db9da32f16c461a9e7a77413"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
