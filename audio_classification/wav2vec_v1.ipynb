{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForAudioClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForAudioClassification, Wav2Vec2FeatureExtractor\n",
    "import numpy as np\n",
    "# from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.output.weight', 'classifier.output.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.weight', 'projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 212/212 [00:00<00:00, 32.7kB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://github.com/ehcalabres/EMOVoice\n",
    "# the preprocessor was derived from https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english\n",
    "# processor1 = AutoProcessor.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
    "# ^^^ no preload model available for this model (above), but the `feature_extractor` works in place\n",
    "\n",
    "\n",
    "model1 = AutoModelForAudioClassification.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "        0: \"angry\",\n",
    "        1: \"calm\",\n",
    "        2: \"disgust\",\n",
    "        3: \"fearful\",\n",
    "        4: \"happy\",\n",
    "        5: \"neutral\",\n",
    "        6: \"sad\",\n",
    "        7: \"surprised\"\n",
    "    }\n",
    "\n",
    "label2id = {y:x for x, y in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "crema_id2label = {\n",
    "        0: \"angry\",\n",
    "        2: \"disgust\",\n",
    "        3: \"fear\",\n",
    "        4: \"happy\",\n",
    "        5: \"neutral\",\n",
    "        6: \"sadness\",\n",
    "    }\n",
    "\n",
    "crema_label2id = {y:x for x, y in crema_id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_file):\n",
    "    # if not audio_file:\n",
    "    #     # I fetched some samples with known emotions from here: https://www.fesliyanstudios.com/royalty-free-sound-effects-download/poeple-crying-252\n",
    "    #     audio_file = 'mp3/dude-crying.mp3'\n",
    "\n",
    "    # sound = AudioSegment.from_file(audio_file)\n",
    "    # sound = sound.set_frame_rate(16000)\n",
    "\n",
    "    sig, sr = librosa.load(audio_file)\n",
    "    wav_data = librosa.resample(sig, orig_sr=sr, target_sr=16000)\n",
    "    # sound_array = np.array(wav_data.get_array_of_samples())\n",
    "    \n",
    "    # this model is VERY SLOW, so best to pass in small sections that contain \n",
    "    # emotional words from the transcript. like 10s or less.\n",
    "    # how to make sub-chunk  -- this was necessary even with very short audio files \n",
    "    # test = torch.tensor(input.input_values.float()[:, :100000])\n",
    "\n",
    "    input = feature_extractor(\n",
    "        raw_speech=wav_data,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    result = model1.forward(input.input_values.float())\n",
    "    # making sense of the result \n",
    "    id2label = {\n",
    "        \"0\": \"angry\",\n",
    "        \"1\": \"calm\",\n",
    "        \"2\": \"disgust\",\n",
    "        \"3\": \"fearful\",\n",
    "        \"4\": \"happy\",\n",
    "        \"5\": \"neutral\",\n",
    "        \"6\": \"sad\",\n",
    "        \"7\": \"surprised\"\n",
    "    }\n",
    "    interp = dict(zip(id2label.values(), list(round(float(i),4) for i in result[0][0])))\n",
    "\n",
    "    pred = np.argmax(result[0][0].detach().numpy())\n",
    "    return pred, interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"crema_dataset_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022_ITS_ANG_XX.wav</td>\n",
       "      <td>Crema/1022_ITS_ANG_XX.wav</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1037_ITS_ANG_XX.wav</td>\n",
       "      <td>Crema/1037_ITS_ANG_XX.wav</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1060_ITS_NEU_XX.wav</td>\n",
       "      <td>Crema/1060_ITS_NEU_XX.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1075_ITS_NEU_XX.wav</td>\n",
       "      <td>Crema/1075_ITS_NEU_XX.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1073_IOM_DIS_XX.wav</td>\n",
       "      <td>Crema/1073_IOM_DIS_XX.wav</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file                  file_path    label\n",
       "0  1022_ITS_ANG_XX.wav  Crema/1022_ITS_ANG_XX.wav    angry\n",
       "1  1037_ITS_ANG_XX.wav  Crema/1037_ITS_ANG_XX.wav    angry\n",
       "2  1060_ITS_NEU_XX.wav  Crema/1060_ITS_NEU_XX.wav  neutral\n",
       "3  1075_ITS_NEU_XX.wav  Crema/1075_ITS_NEU_XX.wav  neutral\n",
       "4  1073_IOM_DIS_XX.wav  Crema/1073_IOM_DIS_XX.wav  disgust"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = df.loc[:10, \"file_path\"]\n",
    "labels = df.loc[:10, \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for file in files:\n",
    "    curr_pred = predict_emotion(file)\n",
    "    preds.append(curr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pred = pd.Series(list(zip(*preds))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_id = labels.apply(lambda x: crema_label2id[x])\n",
    "pred_id = raw_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(labels_id, pred_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 2, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/1_dqbwsn08sbtf7s0k5q2lsc0000gn/T/ipykernel_75508/2804918070.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  accuracy = matrix.diagonal()/matrix.sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "accuracy = matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0. , 0.5, 0. , 0. , 0.5, nan])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig, sr = librosa.load(file_path_str)\n",
    "wav_data = librosa.resample(sig, orig_sr=sr, target_sr=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = feature_extractor(\n",
    "    raw_speech=wav_data,\n",
    "    sampling_rate=16000,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model1.forward(input.input_values.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0625,  0.1069, -0.0362, -0.0244,  0.0209,  0.0692, -0.0219,  0.0375]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "        \"0\": \"angry\",\n",
    "        \"1\": \"calm\",\n",
    "        \"2\": \"disgust\",\n",
    "        \"3\": \"fearful\",\n",
    "        \"4\": \"happy\",\n",
    "        \"5\": \"neutral\",\n",
    "        \"6\": \"sad\",\n",
    "        \"7\": \"surprised\"\n",
    "    }\n",
    "interp = dict(zip(id2label.values(), list(round(float(i),4) for i in result[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 0.0625,\n",
       " 'calm': 0.1069,\n",
       " 'disgust': -0.0362,\n",
       " 'fearful': -0.0244,\n",
       " 'happy': 0.0209,\n",
       " 'neutral': 0.0692,\n",
       " 'sad': -0.0219,\n",
       " 'surprised': 0.0375}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sig, sr = librosa.load(audio_file)\n",
    "    wav_data = librosa.resample(sig, orig_sr=sr, target_sr=16000)\n",
    "    sound_array = np.array(wav_data.get_array_of_samples())\n",
    "    \n",
    "    # this model is VERY SLOW, so best to pass in small sections that contain \n",
    "    # emotional words from the transcript. like 10s or less.\n",
    "    # how to make sub-chunk  -- this was necessary even with very short audio files \n",
    "    # test = torch.tensor(input.input_values.float()[:, :100000])\n",
    "\n",
    "    input = feature_extractor(\n",
    "        raw_speech=sound_array,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    result = model1.forward(input.input_values.float())\n",
    "    # making sense of the result \n",
    "    id2label = {\n",
    "        \"0\": \"angry\",\n",
    "        \"1\": \"calm\",\n",
    "        \"2\": \"disgust\",\n",
    "        \"3\": \"fearful\",\n",
    "        \"4\": \"happy\",\n",
    "        \"5\": \"neutral\",\n",
    "        \"6\": \"sad\",\n",
    "        \"7\": \"surprised\"\n",
    "    }\n",
    "    interp = dict(zip(id2label.values(), list(round(float(i),4) for i in result[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"huge_collated_dataset_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_str = df.loc[1, \"file_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig, sr = librosa.load(file_path_str)\n",
    "wav_data = librosa.resample(sig, orig_sr=sr, target_sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/Users/daniel/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:53: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:51\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     52\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/processing_utils.py:184\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mInstantiate a processor associated with a pretrained model.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m        [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/processing_utils.py:228\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         attribute_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m--> 228\u001b[0m     args\u001b[39m.\u001b[39mappend(attribute_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:700\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_py\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    701\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1788\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1789\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1790\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/GitHub/pentahack/audio_classification/wav2vec_v1.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/wav2vec_v1.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/GitHub/pentahack/audio_classification/wav2vec_v1.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForAudioClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/models/auto/processing_auto.py:276\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m PROCESSOR_MAPPING:\n\u001b[0;32m--> 276\u001b[0m     \u001b[39mreturn\u001b[39;00m PROCESSOR_MAPPING[\u001b[39mtype\u001b[39;49m(config)]\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    278\u001b[0m \u001b[39m# At this stage, there doesn't seem to be a `Processor` class available for this model, so let's try a\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m# tokenizer.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:63\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     54\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading a tokenizer inside \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from a config that does not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m include a `tokenizer_class` attribute is deprecated and will be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     62\u001b[0m feature_extractor \u001b[39m=\u001b[39m Wav2Vec2FeatureExtractor\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 63\u001b[0m tokenizer \u001b[39m=\u001b[39m Wav2Vec2CTCTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(feature_extractor\u001b[39m=\u001b[39mfeature_extractor, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/carrosell2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1783\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1784\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1785\u001b[0m     )\n\u001b[1;32m   1787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1788\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1789\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1790\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1796\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('carrosell2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8501986c27cdf10dcff24130cd3590078fda1030db9da32f16c461a9e7a77413"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
